=Index-based search=
==Prerequisites==
Indexing documents is an expensive task. This is especially true for indexing mailboxes. You need to make sure that your current mail back-end has enough capacities left to serve an additional amount of expensive requests. To keep a users mail index in sync with the contents of his mailboxes, they have to be synchronized regularly. This causes the transfer of lots of mail data especially in the initial phase of the indexing process. As this causes also a high load on your OX back-end machines, it is highly recommended to setup separate indexing nodes. These are OX back-end machines that do not respond to API requests but only perform background jobs for keeping the indexes in sync.

It is also necessary that all your back-end nodes form a common hazelcast cluster. See [[Running a cluster]] for details on this. Additionally all indexing nodes must accept RMI requests from other nodes of your cluster. The actual indexes must be visible to all indexing nodes. In a setup with more than one indexing node you will have to use a network storage for this. The storage must be mounted on every indexing node at the same place in the file system.

You also have to increase the possible number of open files on the indexing nodes. A physical index is distributed across multiple files and lots of them are kept open while an index is active. The according configuration option can be found in <code>/opt/open-xchange/etc/ox-scriptconf.sh</code>.

{{InstallPlugin|pluginname=open-xchange-indexing| toplevel=products | sopath=appsuite/stable/backend|version=App Suite}}

After the installation of <code>open-xchange-indexing</code> was successful, you have to restart your OX node.

==Configuration==
===Available configuration files===
For configuring the indexing feature you have to touch multiple configuration options. The current default settings cause the whole feature to stay disabled. All files contain some inline documentation that is worth to be read.

* /opt/open-xchange/etc/index.properties
* /opt/open-xchange/etc/contextSets/index.yml
* /opt/open-xchange/etc/solr.properties
* /opt/open-xchange/etc/indexing-service.properties
* /opt/open-xchange/etc/smal.properties


===Available administration tools===
The indexing feature comes along with some tools for administration purposes. These tools are available as command line tools as well as MBeans that can be utilized via JMX.

'''CoreStore administration'''<br />
As the indexing implemation is based on [http://lucene.apache.org/solr Apache Solr] that in turn is based on [http://lucene.apache.org/core Apache Lucene], the physical indexes are bound to so called ''Solr Cores'' at runtime. These indexes have to be stored within the file system. Such a place in the file system is called a ''CoreStore''. Before that first index can be created you have to register one ore more CoreStores to the OX back-end. For every CoreStore you have to define its URI and the number of Solr Cores that it may handle. Please keep in mind that there may be several Solr Cores for each user of your environment. At least there are two Solr Cores per user but for every additional module that will be indexed, another Solr Core per user will be created. The intention of this is to keep the single Solr Cores as small as possible to increase indexing and searching times as well as memory footprints. Additionally a Solr Core will never contain documents that are not owned by the according user. So, if you have a large number of users, consider to distribute the number of physical indexes between several CoreStores for better performance, but this depends heavily on the used storage and file systems.

You can use four different commands to administrate the CoreStores. These are located under <code>/opt/open-xchange/sbin</code>. You may call each of these tools with the ''-h'' parameter to see an explanation how to use them:
* registercorestore
* unregistercorestore
* listcorestore
* changecorestore


Additionally you may administrate the CoreStores via JMX. See the JavaDoc of <code>com.openexchange.solr.SolrMBean</code> for details.

'''Index Locking'''<br />
An index can be locked to prevent the delagation of any requests to it. This can be useful to remove a physical index during runtime. Under <code>/opt/open-xchange/sbin</code> you will find the tools <code>lockindex</code> and <code>unlockindex</code>. You may call each of these tools with the ''-h'' parameter to see an explanation how to use them.

Additionally you may lock or unlock Indexes via JMX. See the JavaDoc of <code>com.openexchange.solr.SolrMBean</code> for details.

===Sample configuration===
Assume you have a small environment with one OX back-end machine serving some hundred users and want to allow indexing of the primary mail account to every user. At first you have to set up a second back-end node that will not respond to HTTP API requests. So don't mention this machine in your load balancer configuration. You need to make sure that your existing node and this new one form a hazelcast-cluster. This can be verified with <code>/opt/open-xchange/sbin/showruntimestats -c</code>. Hazelcast must be bound to an appropriate network interface on each node. Every node must be able to connect every other node via the IP address that is known to hazelcast.

# Open <code>/opt/open-xchange/etc/index.properties</code> on both nodes and change <code>com.openexchange.index.allowedModules =</code> to <code>com.openexchange.index.allowedModules = 19</code>. According to our [[HTTP_API#LinkObject| HTTP API]], <code>19</code> is the identifier of the mail module. If you change this property in the index.properties file, it becomes valid for all users within your environment. You can also allow indexing on a much finer granularity. See the next subsection for this.
# The constraint that only the primary mail accounts will be indexed is set in <code>/opt/open-xchange/etc/smal.properties</code>. Here you can define a blacklist of mail account providers. The blacklist is based on the mail servers host names. The default configuration <code>*</code> causes all external mail accounts to be blacklisted. This setting must also be identical on all of your back-end nodes.
# Now open <code>/opt/open-xchange/etc/indexing-service.properties</code> on your dedicated indexing node only and change <code>com.openexchange.service.indexing.workerThreads</code> to the number of CPU cores on this node. Based on the load that occurs on the indexing node and your mail backend during runtime you may increase this number to allow more concurrent synchronization jobs. Please don't forget to restart the node after a change of this property.
# Edit <code>/opt/open-xchange/etc/solr.properties</code> then on your indexing node and change <code>com.openexchange.solr.isSolrNode</code> to <code>true</code>. This configures the node as an indexing node and activates the processing of indexing jobs.
# Additionally you have to make sure that the indexing node publishes its RMI registry. Nodes that handle HTTP API requests will delegate search requests via RMI to the indexing nodes. Therefore you have to bind the registry to an interface that can be reached by all other nodes. This can be configured in <code>/opt/open-xchange/etc/rmi.properties</code>
# At least you should increase the possible number of open files on the indexing node. Open <code>/opt/open-xchange/etc/ox-scriptconf.sh</code> and change <code>NRFILES</code> to <code>65536</code>.


After configuring your indexing node you have to setup your first CoreStore. If you plan to use more than one indexing node you have to use a network storage that can be mounted on the different machines. Create a directory within the file system for every CoreStore on your indexing nodes (and maybe mount your network file storage to them). These directories need write permissions for the user <code>open-xchange</code>:<br />

  $ mkdir /var/opt/corestore
  $ chown open-xchange:open-xchange /var/opt/corestore


Now you have to register your core store. You may do this from your non-indexing-node to prevent from errors on the indexing node. These may occurr because the indexing node will be ready right after its start, but no indexes can be created without an existing CoreStore (Note that 100 cores are enough to index the mail accounts of 50 users).

  $ /opt/open-xchange/sbin/registercorestore -u file:/var/opt/corestore -c 100

Now you can start up your indexing node and log in with a test user. A login-trigger will then schedule indexing jobs for your mailboxes.

===Utilizing the config cascade===
See [[ConfigCascade|ConfigCascade]] for detailed information on this. The config cascade can be used to allow indexing for sets of contexts, single contexts and even single users.

* To allow indexing for a set of contexts, edit <code>/opt/open-xchange/etc/contextSets/index.yml</code> and set the <code>com.openexchange.index.allowedModules = 19</code> accordingly to what you want to allow. If your contexts are already grouped by tags you may want to reuse one of these tags. In this case also change the <code>withTags</code>-property to the according tags. If not you may tag existing contexts with the tag <code>index-search</code>, using the <code>changecontext</code> command.
* To allow mail indexing for a single context or user make use of the <code>changecontext/changeuser</code> command and set the option <code>--config/com.openexchange.index.allowedModules=19</code>.

==Architectural details==
===Solr Core handling===
As mentioned above, a physical index is represented as a Solr Core at runtime. In the whole cluster only one Solr Core is allowed to be active for one physical index at the same time. This is caused by Apache Solrs locking behavior and led to some noteworthy architectural decisions:

* Only OX nodes that are configured to be indexing nodes can start up Solr Cores.
* Starting / stopping a Solr Core utilizes a cluster-wide distributed lock. We use Hazelcast for this. We also use a Hazelcast-based distributed collection to remember, on which node a Solr Core is active.
* Adding or removing documents to / from an index as well as searching needs to be delegated to the OX node that owns the corresponding Solr Core. We use RMI for this delegation. This causes the need to bind the RMI registry of an indexing node to its external network interface.
* The decision on which node a Solr Core will be started is made during runtime and considers the number of already active cores on each indexing node. This is the cause why the Core Stores must be mounted on every indexing node.
* Solr Cores are started on demand. This can be a search request initiated by a user or an indexing job that tries to add or delete some documents. A Solr Core that was not used for a defined amount of time will be shut down to free its corresponding resources (mostly memory and file descriptors).


===Indexing jobs===
Indexing a users data (e.g. mailboxes) is done using background jobs. We use a cluster-wide job scheduler for this that is based on [http://quartz-scheduler.org Quartz Enterprise Job Scheduler] and Hazelcast. For indexing jobs, all none-indexing nodes act as clients to the scheduler (i.e. they can schedule or un-schedule jobs but will never execute them). In contrast all indexing nodes act as workers that take jobs from the scheduler and execute them. This distinction was made to keep away high load from non-indexing nodes caused by running jobs. After an indexing node took a job from the scheduler, it decides if it executes the job on its own or if it delegates the execution to another indexing node. Again this behavior is a result of the Solr Core handling. A jobs execution will be delegated to another node if the affected Solr Core is active on that node. This avoids a high network load caused by dozens of expensive RMI requests between the node that executes the job and the one that owns the affected Solr Core.

===Example===
Imagine a user logs in and the load balancer delegates his requests to <code>OX 1</code>. The users mail index is located in the physical index <code>Index 1</code>. When the user tries to search in his INBOX, a Solr Core for <code>Index 1</code> needs to be started. This happens on <code>OX 4</code>. Subsequently the search request will be delegated to <code>OX 4</code> via RMI. The search request further causes the scheduling of a synchronization job for the users INBOX. <code>OX 5</code> takes this job from the scheduler and detects, that the affected Solr Core is running on <code>OX 4</code>. It then delegates the jobs execution to this node.
[[File:Indexing_Architecture.png|600px|Indexing Architecture]]

==Monitoring==
At the moment these things can be monitored:
;The list of active Solr Cores per indexing node
: This can be monitored with the CLI <code>/opt/open-xchange/sbin/listactivesolrcores</code>.
: If you use [[OX_munin_scripts | Munin]] for OX Monitoring, you can activate the plugin <code>ox_solr-cores</code>.
: The corresponding MBean is <code>com.openexchange.solr, type=solrControl</code>. See the JavaDoc of <code>com.openexchange.solr.SolrMBean</code> for more details. The MBean contains an attribute called <code>ActiveCores</code> that returns a list of Solr Core names.
;A list of currently running jobs per indexing node
:  This can be monitored with the CLI <code>/opt/open-xchange/sbin/listindexingjobs -r</code>.
: If you use [[OX_munin_scripts | Munin]] for OX Monitoring, you can activate the plugin <code>ox_indexing-jobs</code>.
: The corresponding MBean is <code>com.openexchange.service.indexing, type=indexingServiceMonitoring</code>. See the JavaDoc of <code>com.openexchange.service.indexing.JobMonitoringMBean</code> for more details. The Attribute to list all currently running jobs is <code>RunningJobs</code> and returns a map of trigger names->job names.
;A list of all scheduled indexing jobs in the whole cluster
: This can be monitored with the CLI <code>/opt/open-xchange/sbin/listindexingjobs -s</code>.
: If you use [[OX_munin_scripts | Munin]] for OX Monitoring, you can activate the plugin <code>ox_indexing-jobs</code>.
: This can be monitored using the same MBean as mentioned above. The attribute for this value is <code>StoredJobInfos</code> and returns a list of job ids.

For monitoring the per-node values you have to request these values on the affected node. Cluster-wide values can be received on every node in the cluster.

<!--
==Sizing==
-->
[[Category: OX7]]
[[Category: AppSuite]]
